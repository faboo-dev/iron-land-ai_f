import os
import json
from typing import List, Dict
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ========================================
# 1. JSON ë¬¸ì„œ ë¡œë“œ
# ========================================
def load_json_documents(data_dir: str = "./data") -> List[Document]:
    """
    JSON íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
    """
    documents = []
    
    for filename in os.listdir(data_dir):
        if not filename.endswith('.json'):
            continue
            
        filepath = os.path.join(data_dir, filename)
        print(f"ğŸ“‚ Loading: {filename}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            metadata = {
                "id": item.get("id"),
                "source_type": item.get("source_type"),
                "title": item.get("title"),
                "original_url": item.get("original_url"),
                "url_full": item.get("url_full"),
                "timestamp_str": item.get("timestamp_str"),
                "timestamp_seconds": item.get("timestamp_seconds", 0),
                "source_file": filename,
            }
            
            doc = Document(
                page_content=item.get("raw_content", ""),
                metadata=metadata
            )
            documents.append(doc)
    
    print(f"âœ… Loaded {len(documents)} documents")
    return documents


# ========================================
# 2. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
# ========================================
def smart_chunking(documents: List[Document], 
                   window_size: int = 3,
                   max_chunk_size: int = 1200) -> List[Document]:
    """
    ì—°ê´€ëœ íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ì²­í¬ë¥¼ ë³‘í•©
    """
    merged_docs = []
    
    # ê°™ì€ ì˜ìƒë¼ë¦¬ ê·¸ë£¹í™”
    video_groups = {}
    for doc in documents:
        video_id = doc.metadata.get("original_url")
        if video_id not in video_groups:
            video_groups[video_id] = []
        video_groups[video_id].append(doc)
    
    # ê° ì˜ìƒë³„ë¡œ ì²˜ë¦¬
    for video_id, docs in video_groups.items():
        docs.sort(key=lambda x: x.metadata.get("timestamp_seconds", 0))
        
        i = 0
        while i < len(docs):
            merge_docs = docs[i:i + window_size]
            
            merged_content = ""
            all_timestamps = []
            first_metadata = merge_docs[0].metadata.copy()
            
            for doc in merge_docs:
                timestamp = doc.metadata.get("timestamp_str", "00:00")
                all_timestamps.append(timestamp)
                merged_content += f"[{timestamp}] {doc.page_content}\n\n"
                
                if len(merged_content) >= max_chunk_size:
                    break
            
            first_metadata["id"] = f"{first_metadata['id']}_merged_{i}"
            first_metadata["merged_timestamps"] = all_timestamps
            first_metadata["chunk_type"] = "merged"
            
            keywords = extract_keywords(merged_content)
            first_metadata["keywords"] = keywords
            
            merged_doc = Document(
                page_content=merged_content.strip(),
                metadata=first_metadata
            )
            merged_docs.append(merged_doc)
            
            i += max(1, window_size // 2)
    
    print(f"âœ… Created {len(merged_docs)} merged chunks")
    return merged_docs


def extract_keywords(text: str) -> List[str]:
    """
    í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    hopping_keywords = [
        "ì¬ë§ˆí˜¸í•‘", "ì„ ë§ˆí˜¸í•‘", "ì¬ë§ˆ", "ì„ ë§ˆ",
        "í•´ì í˜¸í•‘", "í•´ì ", 
        "í´ëŸ½ì„¸ë¶€", "í´ëŸ½", 
        "í•œë°”ë‹¤", 
        "ë°”ì´í‚¹í˜¸í•‘", "ë°”ì´í‚¹",
        "ë†€ìí˜¸í•‘", "ë†€ì",
        "ë½ë¹Œë¦¬ì§€",
    ]
    
    topic_keywords = [
        "ê°€ì¡±", "ì•„ì´", "ì–´ë¦°ì´", "ìœ ì¹˜ì›",
        "ê°€ê²©", "ë¹„ìš©", "í• ì¸", "ì˜ˆì•½",
        "ì˜¤ì „", "ì˜¤í›„", "ì¶œë°œ",
        "ìŠ¤ë…¸í´ë§", "ë‹¤ì´ë¹™",
        "íë£¨ëš±ì•ˆ", "ë‚ ë£¨ìˆ˜ì•ˆ", "ì˜¬ë‘ê³ ", "íŒë‹¤ë…¼",
    ]
    
    found_keywords = []
    for keyword in hopping_keywords + topic_keywords:
        if keyword in text:
            found_keywords.append(keyword)
    
    return list(set(found_keywords))


# ========================================
# 3. ChromaDB ìƒì„±
# ========================================
def create_vectorstore(documents: List[Document], 
                       persist_directory: str = "./chroma_db") -> Chroma:
    """
    ChromaDB ìƒì„±
    """
    if os.path.exists(persist_directory):
        import shutil
        shutil.rmtree(persist_directory)
        print(f"ğŸ—‘ï¸  Deleted old database")
    
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004"
    )
    
    print("ğŸ”„ Creating vector store...")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name="travel_knowledge_base"
    )
    
    print(f"âœ… Vector store created with {len(documents)} documents")
    return vectorstore


# ========================================
# MAIN
# ========================================
if __name__ == "__main__":
    print("ğŸš€ Starting data ingestion...\n")
    
    # 1. JSON ë¡œë“œ
    documents = load_json_documents("./data")
    
    # 2. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    merged_documents = smart_chunking(documents, window_size=3, max_chunk_size=1200)
    
    # 3. Vector Store ìƒì„±
    vectorstore = create_vectorstore(merged_documents)
    
    print("\nâœ… Ingestion completed!")
