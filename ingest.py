import os
import json
from typing import List, Dict
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ========================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# ========================================
os.environ["GOOGLE_API_KEY"] = "YOUR_GOOGLE_API_KEY"

# ========================================
# 1. JSON ë¬¸ì„œ ë¡œë“œ
# ========================================
def load_json_documents(data_dir: str = "./data") -> List[Document]:
    """
    JSON íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
    ê° íƒ€ì„ìŠ¤íƒ¬í”„ë³„ raw_contentë¥¼ Documentë¡œ ë³€í™˜
    """
    documents = []
    
    for filename in os.listdir(data_dir):
        if not filename.endswith('.json'):
            continue
            
        filepath = os.path.join(data_dir, filename)
        print(f"ğŸ“‚ Loading: {filename}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "id": item.get("id"),
                "source_type": item.get("source_type"),
                "title": item.get("title"),
                "original_url": item.get("original_url"),
                "url_full": item.get("url_full"),
                "timestamp_str": item.get("timestamp_str"),
                "timestamp_seconds": item.get("timestamp_seconds", 0),
                "source_file": filename,
            }
            
            # Document ìƒì„±
            doc = Document(
                page_content=item.get("raw_content", ""),
                metadata=metadata
            )
            documents.append(doc)
    
    print(f"âœ… Loaded {len(documents)} documents")
    return documents


# ========================================
# 2. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ì—°ê´€ ì²­í¬ ë³‘í•©)
# ========================================
def smart_chunking(documents: List[Document], 
                   window_size: int = 3,
                   max_chunk_size: int = 1200) -> List[Document]:
    """
    ì—°ê´€ëœ íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ì²­í¬ë¥¼ ë³‘í•©
    
    Args:
        documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        window_size: ë³‘í•©í•  ì—°ì† ì²­í¬ ê°œìˆ˜ (ê¸°ë³¸ 3ê°œ)
        max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ê¸°ë³¸ 1200ì)
    
    Returns:
        ë³‘í•©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    merged_docs = []
    
    # ê°™ì€ ì˜ìƒë¼ë¦¬ ê·¸ë£¹í™”
    video_groups = {}
    for doc in documents:
        video_id = doc.metadata.get("original_url")
        if video_id not in video_groups:
            video_groups[video_id] = []
        video_groups[video_id].append(doc)
    
    # ê° ì˜ìƒë³„ë¡œ ì²˜ë¦¬
    for video_id, docs in video_groups.items():
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœì„œë¡œ ì •ë ¬
        docs.sort(key=lambda x: x.metadata.get("timestamp_seconds", 0))
        
        i = 0
        while i < len(docs):
            # window_sizeë§Œí¼ ë³‘í•©
            merge_docs = docs[i:i + window_size]
            
            # ë³‘í•©ëœ ë‚´ìš© ìƒì„±
            merged_content = ""
            all_timestamps = []
            first_metadata = merge_docs[0].metadata.copy()
            
            for doc in merge_docs:
                timestamp = doc.metadata.get("timestamp_str", "00:00")
                all_timestamps.append(timestamp)
                merged_content += f"[{timestamp}] {doc.page_content}\n\n"
                
                # ìµœëŒ€ í¬ê¸° ì²´í¬
                if len(merged_content) >= max_chunk_size:
                    break
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            first_metadata["id"] = f"{first_metadata['id']}_merged_{i}"
            first_metadata["merged_timestamps"] = all_timestamps
            first_metadata["chunk_type"] = "merged"
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
            keywords = extract_keywords(merged_content)
            first_metadata["keywords"] = keywords
            
            # ìƒˆ ë¬¸ì„œ ìƒì„±
            merged_doc = Document(
                page_content=merged_content.strip(),
                metadata=first_metadata
            )
            merged_docs.append(merged_doc)
            
            # ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì´ë™ (50% ì˜¤ë²„ë©)
            i += max(1, window_size // 2)
    
    print(f"âœ… Created {len(merged_docs)} merged chunks from {len(documents)} original docs")
    return merged_docs


def extract_keywords(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
    ì‹¤ì œë¡œëŠ” KoNLPy ë“± ì‚¬ìš© ê¶Œì¥
    """
    # í˜¸í•‘ ì—…ì²´ í‚¤ì›Œë“œ
    hopping_keywords = [
        "ì¬ë§ˆí˜¸í•‘", "ì„ ë§ˆí˜¸í•‘", "ì¬ë§ˆ", "ì„ ë§ˆ",
        "í•´ì í˜¸í•‘", "í•´ì ", 
        "í´ëŸ½ì„¸ë¶€", "í´ëŸ½", 
        "í•œë°”ë‹¤", 
        "ë°”ì´í‚¹í˜¸í•‘", "ë°”ì´í‚¹",
        "ë†€ìí˜¸í•‘", "ë†€ì",
        "ë½ë¹Œë¦¬ì§€",
    ]
    
    # ì£¼ì œ í‚¤ì›Œë“œ
    topic_keywords = [
        "ê°€ì¡±", "ì•„ì´", "ì–´ë¦°ì´", "ìœ ì¹˜ì›",
        "ê°€ê²©", "ë¹„ìš©", "í• ì¸", "ì˜ˆì•½",
        "ì˜¤ì „", "ì˜¤í›„", "ì¶œë°œ",
        "ìŠ¤ë…¸í´ë§", "ë‹¤ì´ë¹™",
        "íë£¨ëš±ì•ˆ", "ë‚ ë£¨ìˆ˜ì•ˆ", "ì˜¬ë‘ê³ ", "íŒë‹¤ë…¼",
    ]
    
    found_keywords = []
    for keyword in hopping_keywords + topic_keywords:
        if keyword in text:
            found_keywords.append(keyword)
    
    return list(set(found_keywords))


# ========================================
# 3. ChromaDBì— ì €ì¥
# ========================================
def create_vectorstore(documents: List[Document], 
                       persist_directory: str = "./chroma_db",
                       collection_name: str = "travel_knowledge_base") -> Chroma:
    """
    ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥
    """
    # ê¸°ì¡´ DB ì‚­ì œ (ì¬êµ¬ì¶•)
    if os.path.exists(persist_directory):
        import shutil
        shutil.rmtree(persist_directory)
        print(f"ğŸ—‘ï¸  Deleted old database: {persist_directory}")
    
    # Embeddings ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004"
    )
    
    # ChromaDB ìƒì„±
    print("ğŸ”„ Creating vector store...")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name=collection_name
    )
    
    print(f"âœ… Successfully created vector store with {len(documents)} documents")
    return vectorstore


# ========================================
# 4. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
# ========================================
def test_search(vectorstore: Chroma, query: str = "ì¬ë§ˆí˜¸í•‘"):
    """
    ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    """
    print(f"\nğŸ” Testing search for: '{query}'")
    print("=" * 80)
    
    # ê²€ìƒ‰ ì‹¤í–‰
    results = vectorstore.similarity_search(query, k=5)
    
    print(f"Found {len(results)} results:\n")
    
    for i, doc in enumerate(results, 1):
        print(f"Result {i}:")
        print(f"  ID: {doc.metadata.get('id')}")
        print(f"  Title: {doc.metadata.get('title')}")
        print(f"  Timestamp: {doc.metadata.get('timestamp_str')}")
        print(f"  URL: {doc.metadata.get('url_full')}")
        print(f"  Keywords: {doc.metadata.get('keywords', [])}")
        print(f"  Content Preview: {doc.page_content[:150]}...")
        print()


# ========================================
# MAIN
# ========================================
if __name__ == "__main__":
    print("ğŸš€ Starting data ingestion...\n")
    
    # 1. JSON ë¡œë“œ
    documents = load_json_documents("./data")
    
    print(f"\nğŸ“Š Original documents stats:")
    print(f"  Total: {len(documents)}")
    print(f"  Avg length: {sum(len(d.page_content) for d in documents) / len(documents):.0f} chars")
    
    # 2. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
    merged_documents = smart_chunking(
        documents, 
        window_size=3,  # 3ê°œ ì—°ì† ì²­í¬ ë³‘í•©
        max_chunk_size=1200
    )
    
    print(f"\nğŸ“Š Merged documents stats:")
    print(f"  Total: {len(merged_documents)}")
    print(f"  Avg length: {sum(len(d.page_content) for d in merged_documents) / len(merged_documents):.0f} chars")
    
    # 3. Vector Store ìƒì„±
    vectorstore = create_vectorstore(merged_documents)
    
    # 4. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_search(vectorstore, "ì¬ë§ˆí˜¸í•‘")
    test_search(vectorstore, "ê°€ì¡± ì—¬í–‰ í˜¸í•‘")
    
    print("\nâœ… Ingestion completed!")
