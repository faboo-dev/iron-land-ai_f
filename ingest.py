import os
import json
from typing import List
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from dotenv import load_dotenv

# ν™κ²½λ³€μ λ΅λ“
load_dotenv()

def load_json_documents(data_dir: str = "./data") -> List[Document]:
    """JSON νμΌμ—μ„ λ¬Έμ„ λ΅λ“"""
    documents = []
    
    for filename in os.listdir(data_dir):
        if not filename.endswith('.json'):
            continue
            
        filepath = os.path.join(data_dir, filename)
        print(f"π“‚ Loading: {filename}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            metadata = {
                "id": item.get("id"),
                "source_type": item.get("source_type"),
                "title": item.get("title"),
                "original_url": item.get("original_url"),
                "url_full": item.get("url_full"),
                "timestamp_str": item.get("timestamp_str"),
                "timestamp_seconds": item.get("timestamp_seconds", 0),
                "source_file": filename,
            }
            
            doc = Document(
                page_content=item.get("raw_content", ""),
                metadata=metadata
            )
            documents.append(doc)
    
    print(f"β… Loaded {len(documents)} documents")
    return documents

def smart_chunking(documents: List[Document], window_size: int = 3, max_chunk_size: int = 1200) -> List[Document]:
    """μ—°κ΄€λ νƒ€μ„μ¤νƒ¬ν”„μ μ²­ν¬λ¥Ό λ³‘ν•©"""
    merged_docs = []
    
    video_groups = {}
    for doc in documents:
        video_id = doc.metadata.get("original_url")
        if video_id not in video_groups:
            video_groups[video_id] = []
        video_groups[video_id].append(doc)
    
    for video_id, docs in video_groups.items():
        docs.sort(key=lambda x: x.metadata.get("timestamp_seconds", 0))
        
        i = 0
        while i < len(docs):
            merge_docs = docs[i:i + window_size]
            
            merged_content = ""
            all_timestamps = []
            first_metadata = merge_docs[0].metadata.copy()
            
            for doc in merge_docs:
                timestamp = doc.metadata.get("timestamp_str", "00:00")
                all_timestamps.append(timestamp)
                merged_content += f"[{timestamp}] {doc.page_content}\n\n"
                
                if len(merged_content) >= max_chunk_size:
                    break
            
            first_metadata["id"] = f"{first_metadata['id']}_merged_{i}"
            first_metadata["merged_timestamps"] = all_timestamps
            first_metadata["chunk_type"] = "merged"
            
            keywords = extract_keywords(merged_content)
            first_metadata["keywords"] = keywords
            
            merged_doc = Document(
                page_content=merged_content.strip(),
                metadata=first_metadata
            )
            merged_docs.append(merged_doc)
            
            i += max(1, window_size // 2)
    
    print(f"β… Created {len(merged_docs)} merged chunks")
    return merged_docs

def extract_keywords(text: str) -> List[str]:
    """ν‚¤μ›λ“ μ¶”μ¶"""
    hopping_keywords = [
        "μ¬λ§νΈν•‘", "μ„ λ§νΈν•‘", "μ¬λ§", "μ„ λ§",
        "ν•΄μ νΈν•‘", "ν•΄μ ", 
        "ν΄λ½μ„Έλ¶€", "ν΄λ½", 
        "ν•λ°”λ‹¤", 
        "λ°”μ΄ν‚ΉνΈν•‘", "λ°”μ΄ν‚Ή",
        "λ†€μνΈν•‘", "λ†€μ",
        "λ½λΉλ¦¬μ§€",
    ]
    
    topic_keywords = [
        "κ°€μ΅±", "μ•„μ΄", "μ–΄λ¦°μ΄", "μ μΉμ›",
        "κ°€κ²©", "λΉ„μ©", "ν• μΈ", "μμ•½",
        "μ¤μ „", "μ¤ν›„", "μ¶λ°",
        "μ¤λ…Έν΄λ§", "λ‹¤μ΄λΉ™",
        "νλ£¨λ±μ•", "λ‚ λ£¨μμ•", "μ¬λ‘κ³ ", "νλ‹¤λ…Ό",
    ]
    
    found_keywords = []
    for keyword in hopping_keywords + topic_keywords:
        if keyword in text:
            found_keywords.append(keyword)
    
    return list(set(found_keywords))

def create_vectorstore(documents: List[Document], persist_directory: str = "./chroma_db") -> Chroma:
    """ChromaDB μƒμ„±"""
    if os.path.exists(persist_directory):
        import shutil
        shutil.rmtree(persist_directory)
        print(f"π—‘οΈ  Deleted old database")
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    print("π”„ Creating vector store...")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name="travel_knowledge_base"
    )
    
    print(f"β… Vector store created with {len(documents)} documents")
    return vectorstore

if __name__ == "__main__":
    print("π€ Starting data ingestion...\n")
    
    documents = load_json_documents("./data")
    merged_documents = smart_chunking(documents, window_size=3, max_chunk_size=1200)
    vectorstore = create_vectorstore(merged_documents)
    
    print("\nβ… Ingestion completed!")
