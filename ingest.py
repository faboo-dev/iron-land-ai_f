import os
import json
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
DATA_DIR = "./data"
PERSIST_DIRECTORY = "./chroma_db"

def load_json_documents(data_dir):
    """JSON νμΌμ—μ„ λ¬Έμ„ λ΅λ“"""
    documents = []
    if not os.path.exists(data_dir):
        print(f"β Data directory {data_dir} does not exist.")
        return []

    for filename in os.listdir(data_dir):
        if filename.endswith(".json"):
            filepath = os.path.join(data_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    if isinstance(data, list):
                        for item in data:
                            content = item.get('raw_content', '')
                            if not content:
                                continue
                            
                            # λ©”νƒ€λ°μ΄ν„°μ— λ¨λ“  μ •λ³΄ ν¬ν•¨ (μ νλΈ λ§ν¬, νƒ€μ„μ¤νƒ¬ν”„ λ“±)
                            metadata = {
                                "source": filename,
                                "source_type": item.get('source_type', 'unknown'),
                                "title": item.get('title', ''),
                                "url": item.get('url_full', ''),
                                "url_full": item.get('url_full', ''),
                                "original_url": item.get('original_url', ''),
                                "timestamp_str": item.get('timestamp_str', ''),
                                "timestamp_seconds": item.get('timestamp_seconds', 0),
                                "id": item.get('id', '')
                            }
                            
                            documents.append(Document(page_content=content, metadata=metadata))
                            
                            # λ””λ²„κΉ…: νΉμ • ν‚¤μ›λ“ ν™•μΈ
                            if "μ¬λ§νΈν•‘" in content or "μ„ λ§νΈν•‘" in content:
                                print(f"β… Found 'μ¬λ§νΈν•‘' in {filename} (ID: {item.get('id')})")
                    
                    elif isinstance(data, dict):
                        content = data.get('raw_content', '')
                        if content:
                            metadata = {
                                "source": filename,
                                "source_type": data.get('source_type', 'unknown'),
                                "title": data.get('title', ''),
                                "url": data.get('url_full', ''),
                                "url_full": data.get('url_full', ''),
                                "original_url": data.get('original_url', ''),
                                "timestamp_str": data.get('timestamp_str', ''),
                                "timestamp_seconds": data.get('timestamp_seconds', 0),
                                "id": data.get('id', '')
                            }
                            documents.append(Document(page_content=content, metadata=metadata))
                        
            except Exception as e:
                print(f"β Error loading {filename}: {e}")
    
    return documents

def ingest_data():
    """λ°μ΄ν„° μΈμ μ¤νΈ λ©”μΈ ν•¨μ"""
    
    if not os.getenv("GOOGLE_API_KEY"):
        print("β Error: GOOGLE_API_KEY not found in .env file.")
        return

    print(f"π“‚ Loading data from {DATA_DIR}...")
    docs = load_json_documents(DATA_DIR)
    
    if not docs:
        print("β No documents found to ingest.")
        return

    print(f"β… Found {len(docs)} documents.")
    
    # ν…μ¤νΈ λ¶„ν•  - κ°μ„ λ μ„¤μ •
    print("β‚οΈ Splitting documents into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,        # 500 β†’ 1500 (3λ°° μ¦κ°€)
        chunk_overlap=300,      # 100 β†’ 300 (3λ°° μ¦κ°€)
        separators=["\n\n", "\n", ". ", " ", ""],  # λ¬Έμ¥ λ‹¨μ„ μ°μ„ 
        length_function=len,
    )
    
    split_docs = text_splitter.split_documents(docs)
    
    print(f"β… Created {len(split_docs)} chunks from {len(docs)} documents.")
    
    if split_docs:
        print(f"π“ First chunk length: {len(split_docs[0].page_content)} characters")
        print(f"π“ First chunk preview: {split_docs[0].page_content[:100]}...")
    
    # λ²΅ν„° μ„λ² λ”© μƒμ„±
    print("π”® Creating embeddings with Google Gemini...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # Chroma DBμ— μ €μ¥
    print("π’Ύ Saving to ChromaDB...")
    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings,
        persist_directory=PERSIST_DIRECTORY,
        collection_name="travel_knowledge_base"
    )
    
    print(f"β… Successfully ingested {len(split_docs)} chunks into {PERSIST_DIRECTORY}")
    print(f"π“ Total documents in collection: {vectorstore._collection.count()}")
    
    # ν…μ¤νΈ: "μ¬λ§νΈν•‘" κ²€μƒ‰
    print("\nπ” Testing search for 'μ¬λ§νΈν•‘'...")
    test_results = vectorstore.similarity_search("μ¬λ§νΈν•‘", k=3)
    print(f"Found {len(test_results)} results:")
    for i, doc in enumerate(test_results, 1):
        print(f"\n--- Result {i} ---")
        print(f"Content: {doc.page_content[:150]}...")
        print(f"Source: {doc.metadata.get('source')}")
        if "μ¬λ§νΈν•‘" in doc.page_content or "μ„ λ§νΈν•‘" in doc.page_content:
            print("β… Contains 'μ¬λ§νΈν•‘'!")

if __name__ == "__main__":
    print("=" * 60)
    print("π° Iron Land AI - Data Ingestion (Enhanced)")
    print("=" * 60)
    ingest_data()
    print("\nβ¨ Ingestion complete!")
