import os
from typing import List, Dict
from difflib import SequenceMatcher
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# FastAPI ì•±
app = FastAPI(title="Iron Land Travel AI")

# Vector Store & LLM ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
embeddings = None
vectorstore = None
llm = None

def init_services():
    """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì²« ìš”ì²­ ì‹œ)"""
    global embeddings, vectorstore, llm
    
    if embeddings is None:
        print("ğŸ”„ Initializing embeddings...")
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    if llm is None:
        print("ğŸ”„ Initializing LLM...")
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            temperature=0.3,
            max_tokens=2048
        )
    
    if vectorstore is None:
        print("ğŸ”„ Initializing vector store...")
        persist_dir = "./chroma_db"
        
        if not os.path.exists(persist_dir):
            print(f"âš ï¸  Warning: {persist_dir} not found.")
            print("ğŸ“ Creating empty vector store...")
            
            dummy_docs = [
                Document(
                    page_content="ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”. ingest.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.",
                    metadata={"id": "system_init", "title": "System"}
                )
            ]
            vectorstore = Chroma.from_documents(
                documents=dummy_docs,
                embedding=embeddings,
                collection_name="travel_knowledge_base"
            )
        else:
            vectorstore = Chroma(
                persist_directory=persist_dir,
                embedding_function=embeddings,
                collection_name="travel_knowledge_base"
            )
            print("âœ… Vector store loaded")

# í‚¤ì›Œë“œ ì •ê·œí™”
KEYWORD_NORMALIZATION = {
    "ì¬ë§ˆí˜¸í•‘": ["ì¬ë§ˆí˜¸í•‘", "ì„ ë§ˆí˜¸í•‘", "ì¬ë§ˆ", "ì„ ë§ˆ", "ì¨ë§ˆ", "ì„¬ë§ˆ"],
    "í•´ì í˜¸í•‘": ["í•´ì í˜¸í•‘", "í•´ì ", "í•´ì €í•‘"],
    "í´ëŸ½ì„¸ë¶€": ["í´ëŸ½ì„¸ë¶€", "í´ëŸ½", "ì„¸ë¶€í´ëŸ½"],
    "í•œë°”ë‹¤": ["í•œë°”ë‹¤", "í•œ ë°”ë‹¤"],
}

def normalize_keywords(text: str) -> List[str]:
    """í‚¤ì›Œë“œ ì •ê·œí™”"""
    normalized = []
    text_lower = text.lower()
    
    for base_keyword, variants in KEYWORD_NORMALIZATION.items():
        for variant in variants:
            if variant.lower() in text_lower:
                normalized.extend(variants)
                break
    
    basic_keywords = text.split()
    stopwords = ["ì—", "ëŒ€í•´", "ì•Œë ¤ì¤˜", "ë­ì•¼", "ì–´ë•Œ"]
    basic_keywords = [w for w in basic_keywords if w not in stopwords and len(w) > 1]
    
    normalized.extend(basic_keywords)
    return list(set(normalized))

def hybrid_search(query: str, k: int = 30) -> List[Document]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    init_services()
    
    keywords = normalize_keywords(query)
    print(f"ğŸ” Keywords: {keywords}")
    
    vector_results = vectorstore.similarity_search(query, k=k)
    
    keyword_results = []
    all_docs = vectorstore.get()
    
    if all_docs and 'metadatas' in all_docs and 'documents' in all_docs:
        for metadata, content in zip(all_docs['metadatas'], all_docs['documents']):
            doc_keywords = metadata.get('keywords', [])
            if any(kw in doc_keywords for kw in keywords) or any(kw in content for kw in keywords):
                keyword_results.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
    
    combined = []
    seen_ids = set()
    
    for doc_list in [keyword_results, vector_results]:
        for doc in doc_list:
            doc_id = doc.metadata.get('id')
            if doc_id and doc_id not in seen_ids:
                combined.append(doc)
                seen_ids.add(doc_id)
    
    return combined[:20]

PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì² ì‚°ëœë“œ ì—¬í–‰ ì •ë³´ AIì…ë‹ˆë‹¤.

[ì ˆëŒ€ ê·œì¹™]
1. [Context]ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì‘ì„± ê¸ˆì§€
2. ì¶”ë¡ /ì¶”ì¸¡ í‘œí˜„ ê¸ˆì§€
3. ê° ë¬¸ì¥ë§ˆë‹¤ ì¶œì²˜ ë§í¬ í•„ìˆ˜

[Context]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€ í˜•ì‹]
## ì² ì‚°ëœë“œ ì—¬í–‰ ê¸°ë¡
(Context ê¸°ë°˜ ë‹µë³€, ì¶œì²˜ ë§í¬ í¬í•¨)

## AI ì¼ë°˜ ì§€ì‹
(ì¼ë°˜ì ì¸ ì—¬í–‰ ì •ë³´)
"""

def generate_answer(query: str) -> Dict:
    """ë‹µë³€ ìƒì„±"""
    init_services()
    
    retrieved_docs = hybrid_search(query, k=30)
    
    context_parts = []
    for i, doc in enumerate(retrieved_docs[:10], 1):
        title = doc.metadata.get('title', 'ì˜ìƒ')
        timestamp = doc.metadata.get('timestamp_str', '00:00')
        url = doc.metadata.get('url_full', '')
        content = doc.page_content[:500]
        
        source = f"[{title} ({timestamp})]({url})" if url else f"{title} ({timestamp})"
        context_parts.append(f"[Document {i}]\nì¶œì²˜: {source}\në‚´ìš©: {content}\n")
    
    context = "\n".join(context_parts)
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=PROMPT_TEMPLATE
    )
    
    final_prompt = prompt.format(context=context, question=query)
    response = llm.invoke(final_prompt)
    
    return {
        "answer": response.content,
        "sources": [
            {
                "title": doc.metadata.get('title'),
                "timestamp": doc.metadata.get('timestamp_str'),
                "url": doc.metadata.get('url_full'),
            }
            for doc in retrieved_docs[:10]
        ]
    }

class ChatRequest(BaseModel):
    query: str

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict]

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±„íŒ… API"""
    try:
        result = generate_answer(request.query)
        return ChatResponse(**result)
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "message": "Iron Land Travel AI is running!",
        "status": "healthy"
    }

@app.get("/health")
async def health():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    init_services()
    
    return {
        "status": "healthy",
        "embeddings": "initialized" if embeddings else "not initialized",
        "llm": "initialized" if llm else "not initialized",
        "vectorstore": "initialized" if vectorstore else "not initialized"
    }

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Starting Iron Land Travel AI...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
