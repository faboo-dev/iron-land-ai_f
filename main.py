from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import os
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Load environment variables
load_dotenv()

app = FastAPI()

# Initialize Vector DB (Chroma) with Gemini Embeddings
PERSIST_DIRECTORY = "./chroma_db"
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

vectorstore = Chroma(
    persist_directory=PERSIST_DIRECTORY,
    embedding_function=embeddings,
    collection_name="travel_knowledge_base"
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

import google.generativeai as genai

# Configure GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Initialize LLM (Google Gemini)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7,
    max_tokens=1024,
    max_retries=2,
)

@app.on_event("startup")
async def startup_event():
    print("Listing available models...")
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"Found model: {m.name}")
    except Exception as e:
        print(f"Error listing models: {e}")

# RAG Prompt
template = """ë‹¹ì‹ ì€ 'ì² ì‚°ëœë“œ(Iron Land)'ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

[Context]:
{context}

[Question]:
{question}

[Guidelines]:
1. **í•µì‹¬ ê·œì¹™**: ì§ˆë¬¸ì— ìˆëŠ” **êµ¬ì²´ì ì¸ ëŒ€ìƒ(ì˜ˆ: í•´ì í˜¸í•‘, ì¬ë§ˆí˜¸í•‘ ë“±)**ì´ [Context]ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´, ì ˆëŒ€ ì–µì§€ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
   - [Context]ì— í•´ë‹¹ ëŒ€ìƒì— ëŒ€í•œ ì–¸ê¸‰ì´ ì—†ë‹¤ë©´, **"ì² ì‚°ëœë“œ ê¸°ë¡ì—ëŠ” '{question}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."**ë¼ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”.
   - ì—‰ëš±í•œ ì •ë³´(ì˜ˆ: ê³ ë˜ìƒì–´ íˆ¬ì–´ ë“±)ë¥¼ ì„ì–´ì„œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   **[ğŸ° ì² ì‚°ëœë“œ ê¸°ë¡]**
   - [Context]ì—ì„œ ì§ˆë¬¸ê³¼ **ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì •ë³´**ë§Œ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.
   - ë‹µë³€ ì¤‘ê°„ì¤‘ê°„ì— (ì¶œì²˜: íŒŒì¼ëª…)ì„ ëª…ì‹œí•˜ì„¸ìš”.
   - ì •ë³´ê°€ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ ì—†ë‹¤ê³  ë§í•˜ì„¸ìš”.

   **[ğŸ¤– AI í¬ë¡œìŠ¤ì²´í¬]**
   - [ì² ì‚°ëœë“œ ê¸°ë¡]ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ì„ ê²½ìš°, ë‹¹ì‹ ì˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë³´ì¶© ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”.
   - **ê°€ê²©ì´ë‚˜ ìµœì‹  ì •ë³´**ëŠ” "ì´ ì •ë³´ëŠ” ë³€ë™ë  ìˆ˜ ìˆìœ¼ë‹ˆ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•´ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤."ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.
   - ë§Œì•½ [ì² ì‚°ëœë“œ ê¸°ë¡]ì— ì •ë³´ê°€ ì•„ì˜ˆ ì—†ë‹¤ë©´, ì—¬ê¸°ì„œ ë‹¹ì‹ ì´ ì•„ëŠ” ë‚´ìš©ì„ ìµœëŒ€í•œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

3. **í†¤ì•¤ë§¤ë„ˆ**: ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
"""
prompt = ChatPromptTemplate.from_template(template)

# Load all documents for Keyword Search (Substring Match)
print("Loading all documents for Keyword Search...")
all_docs_data = vectorstore.get()
all_contents = all_docs_data['documents']
all_metadatas = all_docs_data['metadatas']

from langchain_core.documents import Document
cached_docs = []
for i, content in enumerate(all_contents):
    metadata = all_metadatas[i] if all_metadatas else {}
    cached_docs.append(Document(page_content=content, metadata=metadata))
print(f"Loaded {len(cached_docs)} documents for Keyword Search.")

def retrieve_combined(query):
    # 1. Keyword Search (Substring)
    keyword_docs = []
    # Simple heuristic: only do substring search if query is short enough to be a keyword
    # or just always do it. Always do it for robustness.
    for doc in cached_docs:
        if query in doc.page_content:
            keyword_docs.append(doc)
    
    # 2. Vector Search
    vector_docs = retriever.invoke(query)
    
    # 3. Combine & Deduplicate
    seen_content = set()
    final_docs = []
    
    # Prioritize keyword matches
    for doc in keyword_docs:
        if doc.page_content not in seen_content:
            final_docs.append(doc)
            seen_content.add(doc.page_content)
            
    # Add vector matches
    for doc in vector_docs:
        if doc.page_content not in seen_content:
            final_docs.append(doc)
            seen_content.add(doc.page_content)
    
    # Limit to k=10
    return final_docs[:10]

from langchain_core.runnables import RunnableLambda

# RAG Chain
def format_docs(docs):
    formatted_docs = []
    for d in docs:
        source = d.metadata.get('source', 'Unknown')
        content = d.page_content
        formatted_docs.append(f"Source: {source}\nContent: {content}")
    return "\n\n".join(formatted_docs)

rag_chain = (
    {"context": RunnableLambda(retrieve_combined) | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

class ChatRequest(BaseModel):
    query: str
    history: Optional[List[dict]] = None

class ChatResponse(BaseModel):
    answer: str
    sources: List[dict]

@app.get("/")
def read_root():
    return {"status": "ok", "message": "Travel RAG API is running (Gemini Powered)"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # Retrieve documents
        docs = retriever.invoke(request.query)
        
        # Generate answer
        answer = rag_chain.invoke(request.query)
        
        # Extract sources with details
        sources = []
        for doc in docs:
            sources.append({
                "source": doc.metadata.get("source", "Unknown"),
                "title": doc.metadata.get("title", ""),
                "url": doc.metadata.get("url", ""),
                "timestamp": doc.metadata.get("timestamp", "")
            })
        
        return ChatResponse(answer=answer, sources=sources)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
