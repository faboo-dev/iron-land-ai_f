from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import os
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Load environment variables
load_dotenv()

app = FastAPI()

# Initialize Vector DB (Chroma) with Gemini Embeddings
PERSIST_DIRECTORY = "./chroma_db"
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

vectorstore = Chroma(
    persist_directory=PERSIST_DIRECTORY,
    embedding_function=embeddings,
    collection_name="travel_knowledge_base"
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

import google.generativeai as genai

# Configure GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Initialize LLM (Google Gemini)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7,
    max_tokens=1024,
    max_retries=2,
)

@app.on_event("startup")
async def startup_event():
    print("Listing available models...")
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"Found model: {m.name}")
    except Exception as e:
        print(f"Error listing models: {e}")

# RAG Prompt
template = """ë‹¹ì‹ ì€ 'ì² ì‚°ëœë“œ(Iron Land)'ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

[Context]:
{context}

[Question]:
{question}

[Guidelines]:
1. **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.**
2. ë‹µë³€ì€ ë‹¤ìŒ ë‘ ë¶€ë¶„ìœ¼ë¡œ ëª…í™•íˆ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì„¸ìš”.

   **[ğŸ° ì² ì‚°ëœë“œ ê¸°ë¡]**
   - ì˜¤ì§ ì œê³µëœ [Context]ì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
   - [Context]ì— ì •ë³´ê°€ ì—†ë‹¤ë©´ "ì² ì‚°ëœë“œ ê¸°ë¡ì—ëŠ” ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
   - ë‹µë³€ ì¤‘ê°„ì¤‘ê°„ì— (ì¶œì²˜: íŒŒì¼ëª…)ì„ ì–¸ê¸‰í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì´ì„¸ìš”.

   **[ğŸ¤– AI í¬ë¡œìŠ¤ì²´í¬]**
   - ë‹¹ì‹ ì˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ [ì² ì‚°ëœë“œ ê¸°ë¡]ì˜ ë‚´ìš©ì„ ë³´ì¶©í•˜ê±°ë‚˜ ê²€ì¦í•˜ì„¸ìš”.
   - [Context]ì˜ 'Source'ì— ìˆëŠ” ë‚ ì§œ ì •ë³´(ì˜ˆ: 202507ì€ 2025ë…„ 7ì›”)ë¥¼ í™•ì¸í•˜ê³ , "ì´ ê¸°ë¡ì€ 2025ë…„ 7ì›” ê¸°ì¤€ì…ë‹ˆë‹¤. í˜„ì¬ ì •ë³´ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."ì™€ ê°™ì€ ì£¼ì˜ì‚¬í•­ì„ ì¶”ê°€í•˜ì„¸ìš”.
   - [ì² ì‚°ëœë“œ ê¸°ë¡]ì— ì •ë³´ê°€ ì—†ì—ˆë‹¤ë©´, ì—¬ê¸°ì„œ ë‹¹ì‹ ì´ ì•„ëŠ” ì •ë³´ë¥¼ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
prompt = ChatPromptTemplate.from_template(template)

# Load all documents for Keyword Search (Substring Match)
print("Loading all documents for Keyword Search...")
all_docs_data = vectorstore.get()
all_contents = all_docs_data['documents']
all_metadatas = all_docs_data['metadatas']

from langchain_core.documents import Document
cached_docs = []
for i, content in enumerate(all_contents):
    metadata = all_metadatas[i] if all_metadatas else {}
    cached_docs.append(Document(page_content=content, metadata=metadata))
print(f"Loaded {len(cached_docs)} documents for Keyword Search.")

def retrieve_combined(query):
    # 1. Keyword Search (Substring)
    keyword_docs = []
    # Simple heuristic: only do substring search if query is short enough to be a keyword
    # or just always do it. Always do it for robustness.
    for doc in cached_docs:
        if query in doc.page_content:
            keyword_docs.append(doc)
    
    # 2. Vector Search
    vector_docs = retriever.invoke(query)
    
    # 3. Combine & Deduplicate
    seen_content = set()
    final_docs = []
    
    # Prioritize keyword matches
    for doc in keyword_docs:
        if doc.page_content not in seen_content:
            final_docs.append(doc)
            seen_content.add(doc.page_content)
            
    # Add vector matches
    for doc in vector_docs:
        if doc.page_content not in seen_content:
            final_docs.append(doc)
            seen_content.add(doc.page_content)
    
    # Limit to k=10
    return final_docs[:10]

from langchain_core.runnables import RunnableLambda

# RAG Chain
def format_docs(docs):
    formatted_docs = []
    for d in docs:
        source = d.metadata.get('source', 'Unknown')
        content = d.page_content
        formatted_docs.append(f"Source: {source}\nContent: {content}")
    return "\n\n".join(formatted_docs)

rag_chain = (
    {"context": RunnableLambda(retrieve_combined) | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

class ChatRequest(BaseModel):
    query: str
    history: Optional[List[dict]] = None

class ChatResponse(BaseModel):
    answer: str
    sources: List[dict]

@app.get("/")
def read_root():
    return {"status": "ok", "message": "Travel RAG API is running (Gemini Powered)"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # Retrieve documents
        docs = retriever.invoke(request.query)
        
        # Generate answer
        answer = rag_chain.invoke(request.query)
        
        # Extract sources with details
        sources = []
        for doc in docs:
            sources.append({
                "source": doc.metadata.get("source", "Unknown"),
                "title": doc.metadata.get("title", ""),
                "url": doc.metadata.get("url", ""),
                "timestamp": doc.metadata.get("timestamp", "")
            })
        
        return ChatResponse(answer=answer, sources=sources)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
