import os
from typing import List, Dict, Optional
from difflib import SequenceMatcher
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# ========================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# ========================================
os.environ["AIzaSyDW2umdbsDWSMkIeX7VsdHoRfrcXp_qsYE"] = "AIzaSyDW2umdbsDWSMkIeX7VsdHoRfrcXp_qsYE"

# FastAPI ì•±
app = FastAPI(title="Iron Land Travel AI")

# ========================================
# 1. Vector Store ë¡œë“œ
# ========================================
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="travel_knowledge_base"
)

# LLM ì„¤ì •
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.3,  # í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ë¥¼ ìœ„í•´ ë‚®ì¶¤
    max_tokens=2048
)


# ========================================
# 2. í‚¤ì›Œë“œ ì •ê·œí™”
# ========================================
KEYWORD_NORMALIZATION = {
    "ì¬ë§ˆí˜¸í•‘": ["ì¬ë§ˆí˜¸í•‘", "ì„ ë§ˆí˜¸í•‘", "ì¬ë§ˆ", "ì„ ë§ˆ", "ì¨ë§ˆ", "ì„¬ë§ˆ", "sunma"],
    "í•´ì í˜¸í•‘": ["í•´ì í˜¸í•‘", "í•´ì ", "í•´ì €í•‘", "pirate"],
    "í´ëŸ½ì„¸ë¶€": ["í´ëŸ½ì„¸ë¶€", "í´ëŸ½", "ì„¸ë¶€í´ëŸ½", "club cebu"],
    "í•œë°”ë‹¤": ["í•œë°”ë‹¤", "í•œ ë°”ë‹¤"],
    "ë°”ì´í‚¹": ["ë°”ì´í‚¹í˜¸í•‘", "ë°”ì´í‚¹"],
    "ë†€ì": ["ë†€ìí˜¸í•‘", "ë†€ì"],
    "ë½ë¹Œë¦¬ì§€": ["ë½ë¹Œë¦¬ì§€", "ë½ ë¹Œë¦¬ì§€"],
}

def normalize_keywords(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œí™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    normalized = []
    text_lower = text.lower()
    
    for base_keyword, variants in KEYWORD_NORMALIZATION.items():
        for variant in variants:
            if variant.lower() in text_lower:
                normalized.extend(variants)
                break
    
    # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    basic_keywords = text.split()
    stopwords = ["ì—", "ëŒ€í•´", "ì•Œë ¤ì¤˜", "ë­ì•¼", "ì–´ë•Œ", "ì¸ê°€ìš”", "ëŠ”", "ì„", "ë¥¼"]
    basic_keywords = [w for w in basic_keywords if w not in stopwords and len(w) > 1]
    
    normalized.extend(basic_keywords)
    
    return list(set(normalized))


# ========================================
# 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
# ========================================
def hybrid_search(query: str, k: int = 30) -> List[Document]:
    """
    í‚¤ì›Œë“œ + Vector + í¼ì§€ ë§¤ì¹­ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    """
    # 1. í‚¤ì›Œë“œ ì •ê·œí™”
    keywords = normalize_keywords(query)
    print(f"ğŸ” Normalized keywords: {keywords}")
    
    # 2. Vector ê²€ìƒ‰
    vector_results = vectorstore.similarity_search(query, k=k)
    
    # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (ë©”íƒ€ë°ì´í„° í™œìš©)
    keyword_results = []
    all_docs = vectorstore.get()
    
    if all_docs and 'metadatas' in all_docs and 'documents' in all_docs:
        for i, (metadata, content) in enumerate(zip(all_docs['metadatas'], all_docs['documents'])):
            # ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œ í™•ì¸
            doc_keywords = metadata.get('keywords', [])
            if any(kw in doc_keywords for kw in keywords):
                keyword_results.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
            # ë³¸ë¬¸ í‚¤ì›Œë“œ í™•ì¸
            elif any(kw in content for kw in keywords):
                keyword_results.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
    
    # 4. í¼ì§€ ë§¤ì¹­ (ì˜¤íƒ€ ëŒ€ì‘)
    fuzzy_results = []
    for doc in vector_results:
        for keyword in keywords:
            if len(keyword) < 3:  # ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œëŠ” ìŠ¤í‚µ
                continue
            
            # ë³¸ë¬¸ì—ì„œ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
            words = doc.page_content.split()
            for word in words:
                similarity = SequenceMatcher(None, keyword, word).ratio()
                if similarity >= 0.8 and doc not in fuzzy_results:
                    fuzzy_results.append(doc)
                    break
    
    # 5. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
    combined = []
    seen_ids = set()
    
    # ìš°ì„ ìˆœìœ„: í‚¤ì›Œë“œ > í¼ì§€ > Vector
    for doc_list in [keyword_results, fuzzy_results, vector_results]:
        for doc in doc_list:
            doc_id = doc.metadata.get('id')
            if doc_id and doc_id not in seen_ids:
                combined.append(doc)
                seen_ids.add(doc_id)
    
    print(f"ğŸ“Š Search results: {len(keyword_results)} keyword, {len(fuzzy_results)} fuzzy, {len(vector_results)} vector â†’ {len(combined)} total")
    
    return combined[:20]  # ìƒìœ„ 20ê°œ ë°˜í™˜


# ========================================
# 4. ì¶œì²˜ í¬ë§·íŒ…
# ========================================
def format_source_citation(doc: Document) -> str:
    """
    ì¶œì²˜ ë§í¬ í¬ë§·íŒ…
    """
    title = doc.metadata.get('title', 'ì˜ìƒ')
    timestamp = doc.metadata.get('timestamp_str', '00:00')
    url = doc.metadata.get('url_full', '')
    
    if url:
        return f"[{title} ({timestamp})]({url})"
    else:
        return f"{title} ({timestamp})"


# ========================================
# 5. ì›¹ ê²€ìƒ‰ (ê°€ê²© ì •ë³´)
# ========================================
def web_search_price(query: str) -> str:
    """
    ì‹¤ì‹œê°„ ê°€ê²© ì •ë³´ ì›¹ ê²€ìƒ‰
    ì‹¤ì œë¡œëŠ” Tavily, SerpAPI ë“± ì‚¬ìš©
    ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ êµ¬í˜„
    """
    # TODO: ì‹¤ì œ ì›¹ ê²€ìƒ‰ API ì—°ë™
    if any(keyword in query for keyword in ['ê°€ê²©', 'ë¹„ìš©', 'ì–¼ë§ˆ']):
        return """
ìµœì‹  ê°€ê²© ì •ë³´ (2025ë…„ 12ì›” ê¸°ì¤€):
- í•˜ì´íŠ¸ë˜ë¸”: 100,000ì›
- ë§ˆì´ë¦¬ì–¼íŠ¸ë¦½: 110,000ì›ëŒ€
- ì™€ê·¸: 110,000ì›ëŒ€ (ì¬ë§ˆìŠ¤íŒŒ 1ì‹œê°„ ë¬´ë£Œ)

âš ï¸ ê°€ê²©ì€ ì‹œì¦Œê³¼ í”„ë¡œëª¨ì…˜ì— ë”°ë¼ ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
    return ""


# ========================================
# 6. í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ í”„ë¡¬í”„íŠ¸
# ========================================
ANTI_HALLUCINATION_PROMPT = """ë‹¹ì‹ ì€ ì² ì‚°ëœë“œ ì—¬í–‰ ì •ë³´ AIì…ë‹ˆë‹¤.

[ì ˆëŒ€ ê·œì¹™ - ìœ„ë°˜ ì‹œ ë‹µë³€ ë¬´íš¨]
1. â›” [Context]ì— **ëª…ì‹œì ìœ¼ë¡œ ì í˜€ìˆì§€ ì•Šì€** ë‚´ìš©ì€ **ì ˆëŒ€ ì‘ì„± ê¸ˆì§€**
2. â›” "~ê°™ë‹¤", "~ì¼ ê²ƒì´ë‹¤", "ë³´í†µ ~ì´ë‹¤" ë“±ì˜ **ì¶”ë¡ /ì¶”ì¸¡ í‘œí˜„ ê¸ˆì§€**
3. â›” ì¼ë°˜ì ì¸ ì—¬í–‰ ìƒì‹ì„ **ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”**
4. â›” Contextì— ì—†ëŠ” êµ¬ì²´ì  í”„ë¡œê·¸ë¨/í™œë™ì€ **ì ˆëŒ€ ì–¸ê¸‰ ê¸ˆì§€**

[Context]
{context}

[ì§ˆë¬¸]
{question}

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_search}

[ë‹µë³€ í˜•ì‹]
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

## ğŸ° ì² ì‚°ëœë“œ ì—¬í–‰ ê¸°ë¡

(Contextì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©)
- ê° ì •ë³´ ë’¤ì— ì¶œì²˜ ë§í¬ í•„ìˆ˜: (ì¶œì²˜: [ì˜ìƒ ì œëª© íƒ€ì„ìŠ¤íƒ¬í”„](ë§í¬))
- Contextì— ì—†ëŠ” ë‚´ìš©ì€ "ê¸°ë¡ì— ì—†ìŒ" ëª…ì‹œ
- ìµœì†Œ 5~10ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸íˆ ì‘ì„±
- êµ¬ì²´ì  í”„ë¡œê·¸ë¨/í™œë™ì€ Contextì— ëª…ì‹œëœ ê²ƒë§Œ ì–¸ê¸‰

---

## ğŸ¤– AI ì¼ë°˜ ì§€ì‹ (ì°¸ê³ ìš©)

(ì¼ë°˜ì ì¸ ì„¸ë¶€ í˜¸í•‘íˆ¬ì–´ ì •ë³´)
- ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ê°„ë‹¨íˆ
- "ìœ„ ë‚´ìš©ì€ ì¼ë°˜ ì •ë³´ì´ë©°, ì² ì‚°ëœë“œ ê¸°ë¡ ì°¸ê³ " ëª…ì‹œ

---

## ğŸŒ ìµœì‹  ì •ë³´

{web_search}

---

## ğŸ“ ìš”ì•½

- ì² ì‚°ëœë“œ ê¸°ë¡: (í•µì‹¬ ë‚´ìš©)
- AI ì¼ë°˜ ì§€ì‹: (ë³´ì¶© ì„¤ëª…)
- ìµœì‹  ê°€ê²©: (ê°€ê²© ì •ë³´)
"""

# Prompt Template
prompt_template = PromptTemplate(
    input_variables=["context", "question", "web_search"],
    template=ANTI_HALLUCINATION_PROMPT
)


# ========================================
# 7. RAG Chain
# ========================================
def generate_answer(query: str) -> Dict:
    """
    ìµœì¢… ë‹µë³€ ìƒì„±
    """
    # 1. ê²€ìƒ‰
    retrieved_docs = hybrid_search(query, k=30)
    
    # 2. Context êµ¬ì„±
    context_parts = []
    for i, doc in enumerate(retrieved_docs[:10], 1):  # ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©
        source = format_source_citation(doc)
        content = doc.page_content[:500]  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°
        context_parts.append(f"[Document {i}]\nì¶œì²˜: {source}\në‚´ìš©: {content}\n")
    
    context = "\n".join(context_parts)
    
    # 3. ì›¹ ê²€ìƒ‰ (ê°€ê²© ì •ë³´)
    web_search_result = web_search_price(query)
    
    # 4. LLM í˜¸ì¶œ
    chain = LLMChain(llm=llm, prompt=prompt_template)
    response = chain.run(
        context=context,
        question=query,
        web_search=web_search_result
    )
    
    return {
        "answer": response,
        "sources": [
            {
                "title": doc.metadata.get('title'),
                "timestamp": doc.metadata.get('timestamp_str'),
                "url": doc.metadata.get('url_full'),
            }
            for doc in retrieved_docs[:10]
        ],
        "search_stats": {
            "total_found": len(retrieved_docs),
            "used_in_context": 10
        }
    }


# ========================================
# 8. API Endpoints
# ========================================
class ChatRequest(BaseModel):
    query: str

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict]
    search_stats: Dict

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ì±„íŒ… API
    """
    try:
        result = generate_answer(request.query)
        return ChatResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    return {"message": "Iron Land Travel AI is running!"}


# ========================================
# MAIN
# ========================================
if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ Starting Iron Land Travel AI...")
    print("ğŸ“ Server: http://localhost:8000")
    print("ğŸ“– Docs: http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
