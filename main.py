import os
from typing import List, Dict
from difflib import SequenceMatcher
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv

# ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎìú
load_dotenv()

# FastAPI Ïï±
app = FastAPI(title="Iron Land Travel AI")

# Vector Store & LLM Ï¥àÍ∏∞Ìôî
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="travel_knowledge_base"
)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.3,
    max_tokens=2048
)

# ÌÇ§ÏõåÎìú Ï†ïÍ∑úÌôî
KEYWORD_NORMALIZATION = {
    "Ïç¨ÎßàÌò∏Ìïë": ["Ïç¨ÎßàÌò∏Ìïë", "ÏÑ†ÎßàÌò∏Ìïë", "Ïç¨Îßà", "ÏÑ†Îßà", "Ïç®Îßà", "ÏÑ¨Îßà"],
    "Ìï¥Ï†ÅÌò∏Ìïë": ["Ìï¥Ï†ÅÌò∏Ìïë", "Ìï¥Ï†Å", "Ìï¥Ï†ÄÌïë"],
    "ÌÅ¥ÎüΩÏÑ∏Î∂Ä": ["ÌÅ¥ÎüΩÏÑ∏Î∂Ä", "ÌÅ¥ÎüΩ", "ÏÑ∏Î∂ÄÌÅ¥ÎüΩ"],
    "ÌïúÎ∞îÎã§": ["ÌïúÎ∞îÎã§", "Ìïú Î∞îÎã§"],
}

def normalize_keywords(text: str) -> List[str]:
    """ÌÇ§ÏõåÎìú Ï†ïÍ∑úÌôî"""
    normalized = []
    text_lower = text.lower()
    
    for base_keyword, variants in KEYWORD_NORMALIZATION.items():
        for variant in variants:
            if variant.lower() in text_lower:
                normalized.extend(variants)
                break
    
    basic_keywords = text.split()
    stopwords = ["Ïóê", "ÎåÄÌï¥", "ÏïåÎ†§Ï§ò", "Î≠êÏïº", "Ïñ¥Îïå"]
    basic_keywords = [w for w in basic_keywords if w not in stopwords and len(w) > 1]
    
    normalized.extend(basic_keywords)
    return list(set(normalized))

def hybrid_search(query: str, k: int = 30) -> List[Document]:
    """ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ"""
    keywords = normalize_keywords(query)
    print(f"üîç Keywords: {keywords}")
    
    vector_results = vectorstore.similarity_search(query, k=k)
    
    keyword_results = []
    all_docs = vectorstore.get()
    
    if all_docs and 'metadatas' in all_docs and 'documents' in all_docs:
        for metadata, content in zip(all_docs['metadatas'], all_docs['documents']):
            doc_keywords = metadata.get('keywords', [])
            if any(kw in doc_keywords for kw in keywords) or any(kw in content for kw in keywords):
                keyword_results.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
    
    combined = []
    seen_ids = set()
    
    for doc_list in [keyword_results, vector_results]:
        for doc in doc_list:
            doc_id = doc.metadata.get('id')
            if doc_id and doc_id not in seen_ids:
                combined.append(doc)
                seen_ids.add(doc_id)
    
    return combined[:20]

PROMPT_TEMPLATE = """ÎãπÏã†ÏùÄ Ï≤†ÏÇ∞ÎûúÎìú Ïó¨Ìñâ Ï†ïÎ≥¥ AIÏûÖÎãàÎã§.

[Ï†àÎåÄ Í∑úÏπô]
1. [Context]Ïóê Î™ÖÏãúÎêòÏßÄ ÏïäÏùÄ ÎÇ¥Ïö©ÏùÄ Ï†àÎåÄ ÏûëÏÑ± Í∏àÏßÄ
2. Ï∂îÎ°†/Ï∂îÏ∏° ÌëúÌòÑ Í∏àÏßÄ
3. Í∞Å Î¨∏Ïû•ÎßàÎã§ Ï∂úÏ≤ò ÎßÅÌÅ¨ ÌïÑÏàò

[Context]
{context}

[ÏßàÎ¨∏]
{question}

[ÎãµÎ≥Ä ÌòïÏãù]
## Ï≤†ÏÇ∞ÎûúÎìú Ïó¨Ìñâ Í∏∞Î°ù
(Context Í∏∞Î∞ò ÎãµÎ≥Ä, Ï∂úÏ≤ò ÎßÅÌÅ¨ Ìè¨Ìï®)

## AI ÏùºÎ∞ò ÏßÄÏãù
(ÏùºÎ∞òÏ†ÅÏù∏ Ïó¨Ìñâ Ï†ïÎ≥¥)
"""

def generate_answer(query: str) -> Dict:
    """ÎãµÎ≥Ä ÏÉùÏÑ±"""
    retrieved_docs = hybrid_search(query, k=30)
    
    context_parts = []
    for i, doc in enumerate(retrieved_docs[:10], 1):
        title = doc.metadata.get('title', 'ÏòÅÏÉÅ')
        timestamp = doc.metadata.get('timestamp_str', '00:00')
        url = doc.metadata.get('url_full', '')
        content = doc.page_content[:500]
        
        source = f"[{title} ({timestamp})]({url})" if url else f"{title} ({timestamp})"
        context_parts.append(f"[Document {i}]\nÏ∂úÏ≤ò: {source}\nÎÇ¥Ïö©: {content}\n")
    
    context = "\n".join(context_parts)
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=PROMPT_TEMPLATE
    )
    
    final_prompt = prompt.format(context=context, question=query)
    response = llm.invoke(final_prompt)
    
    return {
        "answer": response.content,
        "sources": [
            {
                "title": doc.metadata.get('title'),
                "timestamp": doc.metadata.get('timestamp_str'),
                "url": doc.metadata.get('url_full'),
            }
            for doc in retrieved_docs[:10]
        ]
    }

class ChatRequest(BaseModel):
    query: str

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict]

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Ï±ÑÌåÖ API"""
    try:
        result = generate_answer(request.query)
        return ChatResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    return {"message": "Iron Land Travel AI is running!"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
